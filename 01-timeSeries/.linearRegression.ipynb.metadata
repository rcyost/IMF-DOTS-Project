{"timestamp": 1646151238.117513, "stored_source_code": "upstream = ['calculateNetworkStats', 'createTimeSeries'] # this means: execute raw.py, then clean.py\nproduct = None\n-\n-\n In [previous work](https://rcyost.github.io/DOTS-network) I've calculated some basic network statistics on IMF Direction of Trade Statistics (DOTS) export data.\n\n In this notebook I'll see if this data has any relationship with percent change bilateral export series. TLDR: currently no linear relationships\n\nTable of Contents:\n 1. Load and clean data\n 2. For each trade series, univariate linear regress bilateral export series against the exporter's network statistics  \n    - This could be re-run on importer's statistics\n    - Recalculate the network with edges as nodes: [example](https://youtu.be/p5LO97n3llg?t=235)\n 3. Sortby pValue, r^2, and aic, check with plots\n 4. Collapse network statistics with PCA, repeat 2,3,4 on PCA series\n\n improvements / future work:\n - dimension reduction\n     - elastic net selection\n     - PCA, DBSCAN,\n - create more features\n     - rolling window\n     - expanding window\n     - lags\n     - use auto feature generation tools:\n         - https://www.featuretools.com/\n         - http://isadoranun.github.io/tsfeat/\n         - http://cesium-ml.org/\n         - https://tsfresh.readthedocs.io/en/latest/text/introduction.html\n         - https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection\n - validate feature importance\n     - check autocorrelations\n     - check multicollinearity\n     - Random Forest importance\n\n - use univariate non-linear models\n - use multivariate models\n - use time series specific models\n - use ml models\n\n\n\n ### 1. Load and clean data\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport statsmodels.api as sm\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntimeSeries=(pd.read_csv(upstream['createTimeSeries']['dotsTimeSeries'])\n    .pivot_table(index='period', columns=['ReferenceArea', 'CounterpartReferenceArea'], values='value')\n)\n\n\ntsPctChange=np.log(timeSeries).pct_change().iloc[1:].dropna(axis=1)\ntsPctChange.columns=['-'.join(col) for col in tsPctChange.columns]\ntsPctChange[tsPctChange>1.5]=np.nan\ntsPctChange[tsPctChange<-1.5]=np.nan\ntsPctChange=tsPctChange.dropna(axis=1)\ntsPctChange.index=pd.to_datetime(tsPctChange.index)\ntsPctChange=tsPctChange[tsPctChange.index > '1985-01-01']\n\n\nnetStats=pd.read_csv(upstream['calculateNetworkStats']['DOTSnetStats']).drop(['Unnamed: 0', 'CONNECTIVITY', 'HAS_BRIDGE', 'TOTAL_NET_VALUE', 'PAGERANK_NUMPY'],axis=1)\nnetStats.set_index(['index', 'PERIOD'], inplace=True)\n# get to period index and econ, stats cols\nnetStatsWide=(netStats\n.reset_index()\n.melt(id_vars=['index', 'PERIOD'])\n.pivot_table(index='PERIOD', columns=['index', 'variable'], values='value')\n)\nnetStatsWide.index = pd.to_datetime(netStatsWide.index)\nnetStatsWidePctChange=netStatsWide.pct_change().iloc[1:].dropna(axis=1)\nnetStatsWidePctChange.index=pd.to_datetime(netStatsWidePctChange.index)\nnetStatsWidePctChange=netStatsWidePctChange[netStatsWidePctChange.index > '1985-01-01']\n\n\nnetStats.corr()\nnetStatsWidePctChange.head()\nnetStatsWidePctChange.corr()\ntsPctChange.head()\nimporters=pd.Series(col.split('-')[0] for col in tsPctChange.columns).unique()\nexporters=pd.Series(col.split('-')[1] for col in tsPctChange.columns).unique()\nallEcons=sorted(set(list(importers) + list(exporters)))\nnetStats=pd.Series(col[1] for col in netStatsWidePctChange.columns).nunique()\n\nprint('The upper-bound on number of tests:', len(allEcons)*netStats)\n## 2. Loop and Linear Regress\n\necons=pd.Series(col for col in tsPctChange.columns).unique()\nregResults=[]\nfor tempSeries in econs:\n\n    # get exporter network data\n    # if country in net stats equals [0] <- exporter, [1] <- importers\n    X_econ=netStatsWidePctChange[[col for col in netStatsWidePctChange.columns if col[0] == tempSeries.split('-')[0]]]\n    # network statistics availiable to exporter\n    allNs=[col[1] for col in X_econ.columns]\n    X_econ.columns=allNs\n\n    # trade import series\n    y=tsPctChange[[tempSeries]]\n    y.columns = ['_'.join(col) for col in y.columns]\n\n    for tempNs in allNs:\n\n        X = X_econ[tempNs]\n        X = sm.add_constant(X, has_constant='add')\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n\n        reg = sm.OLS(y_train, X_train).fit()\n\n        y_pred = reg.predict(X_test)\n\n        tempReturn=(pd.DataFrame({\n            'ns':reg.params.index[1],\n            'coef':reg.params[1],\n            'pvalue':reg.pvalues[1],\n            'r2':reg.rsquared,\n            'aic':reg.aic,\n            'mse':mean_squared_error(y_test, y_pred)},index=[tempSeries])\n        )\n\n        regResults.append(tempReturn)\n\n\n\nregResults=pd.concat(regResults)\nregResults.reset_index(inplace=True)\n ### 3. filter univariate regression results\nregResults[regResults.index.isin(regResults['pvalue'].nsmallest().index)]\n\n\nregResults[regResults.index.isin(regResults['r2'].nlargest().index)]\n\n\nregResults[regResults.index.isin(regResults['aic'].nsmallest().index)]\n\n\nregResults[regResults.index.isin(regResults['mse'].nsmallest().index)]\n\n\nfilteredRegResults=regResults.query('pvalue<0.05 and r2>.5')\nfilteredRegResults.reset_index(drop=True, inplace=True)\nfilteredRegResults\n ### 4. Visual Check\n Let's do a visual check of the series that came back with any remote form of a linear relationship.\n\n We can see that many of the relationships are affected by outliers so these numbers are misleading.\n\nfrom math import ceil\n\n\nncols=4\n\nnrows = ceil(filteredRegResults.shape[0] / ncols)\n\nwidth = ncols * 5\nlength = nrows * 3\n\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, dpi=120, figsize=(width,length))\n\nfor i, ax in enumerate(axes.flatten()):\n    if i < filteredRegResults.shape[0]:\n        ax.scatter(\n            x=tsPctChange[[filteredRegResults['index'][i]]],\n            y=netStatsWidePctChange[[(f\"{filteredRegResults['index'][i].split('-')[0]}\", f\"{filteredRegResults['ns'][i]}\")]])\n\n            # ax.suptitle(f\"{filteredRegResults['index'][i]} Exports to {filteredRegResults['index'][i][1]} and {filteredRegResults['ns'][i]}\")\n        ax.set_title(f\"pvalue:{np.round(filteredRegResults['pvalue'][i], 4)},  r2:{np.round(filteredRegResults['r2'][i], 2)},  aic:{np.round(filteredRegResults['aic'][i], 2)}\")\n        ax.set_ylabel(f\"{filteredRegResults['ns'][i]} Percent Change\")\n        ax.set_xlabel(f\"{filteredRegResults['index'][i]}  Percent Change\")\n    pass\n\nplt.tight_layout()\n## 5. PCA on Network Statistics to Reduce Dimensionality\n\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom numpy.linalg import eig\n\nimporters=pd.Series(col.split('-')[0] for col in tsPctChange.columns).unique()\nexporters=pd.Series(col.split('-')[1] for col in tsPctChange.columns).unique()\nallEcons=sorted(set(list(importers) + list(exporters)))\n\n\nncols=5\nnrows = ceil(len(allEcons) / ncols)\n\nwidth = ncols * 5\nlength = nrows * 3\n\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, dpi=120, figsize=(width,length))\n\n# for i, ax in enumerate(axes.flatten()):\ndef myplot(score,coeff, i, ax, tempSeries, labels=None):\n    xs = score[:,0]\n    ys = score[:,1]\n    n = coeff.shape[0]\n    scalex = 1.0/(xs.max() - xs.min())\n    scaley = 1.0/(ys.max() - ys.min())\n    ax.scatter(xs * scalex,ys * scaley)\n    for i in range(n):\n        ax.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n        if labels is None:\n            ax.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n        else:\n            ax.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n    ax.set_title(tempSeries)\n\n\nfor i, econax in enumerate(zip(allEcons, axes.flatten())):\n\n    tempSeries=econax[0]\n    ax=econax[1]\n\n    # tempSeries.split('-')[0] <- exporter, [1] <- importer\n    temp=netStatsWidePctChange[[col for col in netStatsWidePctChange.columns if col[0] == tempSeries]]\n    # https://stackoverflow.com/questions/50796024/feature-variable-importance-after-a-pca-analysis\n\n    if temp.shape[1] > 0:\n        X = temp\n        #In general a good idea is to scale the data\n        scaler = StandardScaler()\n        scaler.fit(X)\n        X=scaler.transform(X)\n\n        pca = PCA()\n        x_new = pca.fit_transform(X)\n\n        #Call the function. Use only the 2 PCs.\n        myplot(x_new[:,0:2],np.transpose(pca.components_[0:2, :]), i, ax, tempSeries, [col[1] for col in temp])\n\n    plt.tight_layout()\nExample of the Relationship between the original features and the principal components. The values can be interpreted as the correlation between the original feature and the component.\n\necon='Argentina'\ntemp=netStatsWidePctChange[[col for col in netStatsWidePctChange.columns if col[0] == econ]]\nscaler = StandardScaler()\nscaledData = pd.DataFrame(scaler.fit_transform(temp))\n\n\npcaModel = PCA(n_components=3)\n\n\npcaModelFit = pcaModel.fit(scaledData)\nprincipalComponents = pcaModelFit.transform(scaledData)\n\npcaModelFit.explained_variance_ratio_.sum()\n\nloadings = pcaModelFit.components_.T * np.sqrt(pcaModelFit.explained_variance_)\n\nloading_matrix = pd.DataFrame(loadings, index=temp.columns)\nprint(pcaModelFit.explained_variance_ratio_.sum())\nloading_matrix.sort_values(by=[0], ascending=False)\n\n\nan attempt to interpret the principal components:\n\nPC 0: Centrality/Degree measures -> \"Connectivity\"\n\nPC 1: Macro features such as number of edges and nodes while negatively related to pagerank values \n\nPC 2: A bit of everything, overlaps pagerank and number of edges/nodes which are clearly seperated in PC 1\n\necons=pd.Series(col for col in tsPctChange.columns).unique()\nregResultsPCA=[]\nfor tempSeries in econs:\n\n    # network statistics for reference econ\n    X_econ=netStatsWidePctChange[[col for col in netStatsWidePctChange.columns if col[0] == tempSeries.split('-')[0]]]\n\n    # if there is data\n    if X_econ.shape[1] > 0:\n\n        # need to allNs for later\n        allNs=[col[1] for col in X_econ.columns]\n        X_econ.columns=allNs\n\n        scaler = StandardScaler()\n        scaledData = pd.DataFrame(scaler.fit_transform(X_econ))\n\n        #####   PCA\n        # create model\n        n_components=3\n        pcaModel = PCA(n_components=n_components)\n\n        # fit model\n        pcaModelFit = pcaModel.fit(scaledData)\n        X_econ = pd.DataFrame(pcaModelFit.transform(scaledData), columns=[str(col) for col in range(n_components)])\n\n        # trade time series for reference econ\n        y=tsPctChange[[tempSeries]]\n        y.columns = ['_'.join(col) for col in y.columns]\n\n        X_econ.index=y.index\n\n        for tempNs in X_econ.columns:\n            # if tempNs in X.columns:\n            X = X_econ[tempNs]\n            X = sm.add_constant(X, has_constant='add')\n\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n\n            reg = sm.OLS(y_train, X_train).fit()\n\n            y_pred = reg.predict(X_test)\n\n\n            tempReturn=(pd.DataFrame({\n                'ns':reg.params.index[1],\n                'coef':reg.params[1],\n                'pvalue':reg.pvalues[1],\n                'r2':reg.rsquared,\n                'aic':reg.aic,\n                'mse':mean_squared_error(y_test, y_pred)},index=[tempSeries])\n            )\n\n            regResultsPCA.append(tempReturn)\n\nregResultsPCA=pd.concat(regResultsPCA)\nregResultsPCA.reset_index(inplace=True)\n\n\nregResultsPCA[regResultsPCA.index.isin(regResultsPCA['pvalue'].nsmallest().index)]\n\n\nregResultsPCA[regResultsPCA.index.isin(regResultsPCA['r2'].nlargest().index)]\n\n\nregResultsPCA[regResultsPCA.index.isin(regResultsPCA['aic'].nsmallest().index)]\n\n\nregResultsPCA[regResultsPCA.index.isin(regResultsPCA['mse'].nsmallest().index)]\n\n\nregResultsPCA[regResultsPCA.index.isin(abs(regResultsPCA['coef']).nlargest().index)]\n\n\nfilteredregResultsPCA=regResultsPCA.query('pvalue<0.1 and r2>0.2')\nfilteredregResultsPCA.reset_index(drop=True, inplace=True)\nfilteredregResultsPCA\nfrom math import ceil\n\n\nncols=4\n\nnrows = ceil(filteredregResultsPCA.shape[0] / ncols)\n\nwidth = ncols * 5\nlength = nrows * 3\n\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, dpi=120, figsize=(width,length))\n\nfor i, ax in enumerate(axes.flatten()):\n    if i < filteredregResultsPCA.shape[0]:\n        econ=filteredregResultsPCA['index'][i].split('-')[0]\n        temp=netStatsWidePctChange[[col for col in netStatsWidePctChange.columns if col[0] == econ]]\n        if temp.shape[1] > 0:\n            scaler = StandardScaler()\n            scaledData = pd.DataFrame(scaler.fit_transform(temp))\n\n            #####   PCA\n            # create model\n            pcaModel = PCA(n_components=3)\n\n            # fit model\n            pcaModelFit = pcaModel.fit(scaledData)\n            principalComponents = pcaModelFit.transform(scaledData)\n\n            ax.scatter(\n                x=tsPctChange[[filteredregResultsPCA['index'][i]]],\n                #y=netStatsWidePctChange[[(f\"{filteredregResultsPCA['index'][i][0]}\", f\"{filteredregResultsPCA['ns'][i]}\")]])\n                y=pd.DataFrame(principalComponents)[int(filteredregResultsPCA['ns'][i])]\n                )\n\n            # ax.set_suptitle(f\"{filteredregResultsPCA['index'][i][0]} Exports to {filteredregResultsPCA['index'][i][1]} and {filteredregResultsPCA['ns'][i]}\")\n            ax.set_title(f\"pvalue:{np.round(filteredregResultsPCA['pvalue'][i], 5)},  r2:{np.round(filteredregResultsPCA['r2'][i], 2)},  aic:{np.round(filteredregResultsPCA['aic'][i], 2)}\")\n            ax.set_ylabel(f\"{filteredregResultsPCA['ns'][i]} Percent Change\")\n            ax.set_xlabel(f\"{filteredregResultsPCA['index'][i]}\")\n\nplt.tight_layout()\n\n", "params": {}}