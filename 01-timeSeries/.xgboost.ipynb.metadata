{"timestamp": 1647653505.546726, "stored_source_code": "upstream = ['calculateNetworkStats', 'createTimeSeries'] # this means: execute raw.py, then clean.py\nproduct = None\n   In [previous work](https://rcyost.github.io/DOTS-network) I've calculated some basic network statistics on IMF Direction of Trade Statistics (DOTS) export data.\n\n   I looked for relevant features using univariate linear regression [in this notebook](https://rcyost.github.io/network-feature-engineering-trade)\n\n   In this notebook I'll use XGBoost.\n\n  Table of Contents:\n   1. Load and clean data\n   2. For each trade series, XGBoost export series against the exporter's network statistics\n      - This could be re-run on importer's statistics\n      - Recalculate the network with edges as nodes: [example](https://youtu.be/p5LO97n3llg?t=235)\n   3. Sort by mean absolute error.\n   4. Collapse network statistics with PCA, repeat 2,3,4 on PCA series\n\n   ### 1. Load and clean data\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport statsmodels.api as sm\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom numpy import absolute\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\n\n\nfrom tqdm import tqdm\n\nfrom math import ceil\n\nimport pickle\n\ntimeSeries=(pd.read_csv(upstream['createTimeSeries']['dotsTimeSeries'])\n    .pivot_table(index='period', columns=['ReferenceArea', 'CounterpartReferenceArea'], values='value')\n)\n# timeSeries=(pd.read_csv('dotsTimeSeries.csv')\n#     .pivot_table(index='period', columns=['ReferenceArea', 'CounterpartReferenceArea'], values='value')\n# )\n\n\ntsPctChange=np.log(timeSeries).pct_change().iloc[1:].dropna(axis=1)\ntsPctChange.columns=['-'.join(col) for col in tsPctChange.columns]\ntsPctChange[tsPctChange>1.5]=np.nan\ntsPctChange[tsPctChange<-1.5]=np.nan\ntsPctChange=tsPctChange.dropna(axis=1)\ntsPctChange.index=pd.to_datetime(tsPctChange.index)\ntsPctChange=tsPctChange[tsPctChange.index > '1985-01-01']\n\nnetStats=pd.read_csv(upstream['calculateNetworkStats']['DOTSnetStats']).drop(['Unnamed: 0', 'CONNECTIVITY', 'HAS_BRIDGE', 'TOTAL_NET_VALUE', 'PAGERANK_NUMPY'],axis=1)\n# netStats=pd.read_csv('DOTSnetStats.csv').drop(['Unnamed: 0', 'CONNECTIVITY', 'HAS_BRIDGE', 'TOTAL_NET_VALUE', 'PAGERANK_NUMPY'],axis=1)\nnetStats.set_index(['index', 'PERIOD'], inplace=True)\n# get to period index and econ, stats cols\nnetStatsWide=(netStats\n.reset_index()\n.melt(id_vars=['index', 'PERIOD'])\n.pivot_table(index='PERIOD', columns=['index', 'variable'], values='value')\n)\nnetStatsWide.index = pd.to_datetime(netStatsWide.index)\nnetStatsWidePctChange=netStatsWide.pct_change().iloc[1:].dropna(axis=1)\nnetStatsWidePctChange.index=pd.to_datetime(netStatsWidePctChange.index)\nnetStatsWidePctChange=netStatsWidePctChange[netStatsWidePctChange.index > '1985-01-01']\n\n# lag the net stats to not leak information\nnetStatsWidePctChange=netStatsWidePctChange.shift(-1).iloc[:-1]\n# take off a period of time series so sizes match\ntsPctChange=tsPctChange.iloc[:-1]\nnetStats.corr()\nnetStatsWidePctChange.head()\nnetStatsWidePctChange.corr()\ntsPctChange.head()\nimporters=pd.Series(col.split('-')[0] for col in tsPctChange.columns).unique()\nexporters=pd.Series(col.split('-')[1] for col in tsPctChange.columns).unique()\nallEcons=sorted(set(list(importers) + list(exporters)))\nnetStats=pd.Series(col[1] for col in netStatsWidePctChange.columns).nunique()\n\nprint('The upper-bound on number of tests:', len(allEcons)*netStats)\n  ## 2. Loop and XGBoost\n\n# https://www.kaggle.com/felipefiorini/xgboost-hyper-parameter-tuning\n# https://www.kaggle.com/felipefiorini/xgboost-hyper-parameter-tuning/notebook\n\ndef hyperParameterTuning(X_train, y_train):\n    param_tuning = {\n        'learning_rate': [0.01, 0.1],\n        'max_depth': [3, 5, 7, 10],\n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.5, 0.7],\n        'colsample_bytree': [0.5, 0.7],\n        'n_estimators' : [100, 200, 500],\n        'objective': ['reg:squarederror']\n    }\n\n    xgb_model = xgb.XGBRegressor()\n\n    gsearch = GridSearchCV(estimator = xgb_model,\n                           param_grid = param_tuning,\n                           #scoring = 'neg_mean_absolute_error', #MAE\n                           #scoring = 'neg_mean_squared_error',  #MSE\n                           cv = 5,\n                           n_jobs = -1,\n                           verbose = 0)\n\n    gsearch.fit(X_train,y_train)\n\n    return(gsearch.best_params_)\n\n# https://xgboost.readthedocs.io/en/latest/python/examples/index.html\n# https://xgboost.readthedocs.io/en/stable/parameter.html\n# https://www.kaggle.com/stuarthallows/using-xgboost-with-scikit-learn\n\n# https://www.kaggle.com/prashant111/a-guide-on-xgboost-hyperparameters-tuning\n\nresults={}\n\necons=pd.Series(col for col in tsPctChange.columns).unique()\ntempSeries='Argentina-Brazil'\n\n# create dataset\n# network statistics\nX=netStatsWidePctChange[[col for col in netStatsWidePctChange.columns if col[0] == tempSeries.split('-')[0] or col[0] == tempSeries.split('-')[1]]]\nX.columns=[\"-\".join(col) for col in X.columns]\nX_temp=X\n\n# bilateral trade series\ny=tsPctChange[[tempSeries]]\n\n# if there is data for model\nif not X_temp.empty and not y.empty:\n    results[tempSeries]={}\n    results[tempSeries]['y_std']=y.std()\n    results[tempSeries]['series']=tempSeries\n    X_train, X_test, y_train, y_test = train_test_split(X_temp, y, test_size=0.1, shuffle=False)\n    results[tempSeries]['y_test_std']=y_test.std()\n\n    #bestParams=hyperParameterTuning(X_train, y_train)\n\n    bst = xgb.XGBRegressor(\n        objective = 'reg:squarederror',\n        colsample_bytree = .05,\n        learning_rate = .01,\n        max_depth = 3,\n        min_child_weight = 5,\n        n_estimators = 500,\n        subsample = .5,\n        nthread=4)\n\n    #results[tempSeries]['bestParams']=bestParams\n\n    bst.fit(X_train, y_train)\n\n    results[tempSeries]['model']=bst\n\n    y_pred = bst.predict(X_test)\n\n    mse=mean_squared_error(y_test, y_pred)\n    results[tempSeries]['mse']=mse\n\n    results[tempSeries]['data']=[X_train, X_test, y_train, y_test, y_pred]\n\n\n    importances=['weight', 'gain', 'cover']\n    for importance in importances:\n        results[tempSeries][importance]=(bst.get_booster().get_score(importance_type=importance))", "params": {}}